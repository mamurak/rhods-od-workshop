= 2.2 Deploying the Containerized Application

include::_attributes.adoc[]

Now that the model has been deployed, we're ready to deploy its client as a container via creating an OpenShift deployment and have it running as a service which you will be able to call from any other application.

This repository was created as a source-to-image (S2I) Python application. You can see the template https://github.com/opendatahub-io/odh-s2i-project-simple[GitHub repository] and try for yourself.

Some interesting files to examine in the application::

* https://github.com/mamurak/object-detection-rest/blob/master/wsgi.py[wsgi.py] - the main entry point for the python application and calls the prediction function.
* https://github.com/mamurak/object-detection-rest/blob/master/prediction.py[prediction.py] - contains the function the data scientist has extracted to make predictions. Called from the application with a dictionary parameter.
* https://github.com/mamurak/object-detection-rest/blob/master/requirements.txt[requirements.txt] - where the app developer adds their python dependencies for the served application.
* https://github.com/mamurak/object-detection-rest/blob/master/Dockerfile[Dockerfile] - which contains the instructions for how to build the container image of the object detection REST component.

You can also learn more about https://access.redhat.com/documentation/en-us/red_hat_software_collections/2/html/using_red_hat_software_collections_container_images/sti[Source to Image] framework which allows you to write images that use the application source code as an input and produce a new image that runs the assembled application as an output, as well as the https://github.com/sclorg/s2i-python-container[Python builder image].

If you'd like to learn more about building container images with a Dockerfile, check out this https://docs.docker.com/build/guide/[guide].

Now, let's create the application.

== OpenShift Console

We'll start from the *OpenShift Console*.

image::s2i/ocp-console.png[Ocp Console]

NOTE: If you're at the RHOAI Dashboard and need to navigate to the OpenShift Web Console, use the *Application Switcher* Icon in the top right of the navigation bar and click on the *OpenShift Console* menu item.

image::s2i/rhods-dashboard-app-switcher.png[RHOAI Dashboard App Switcher, 400]

* From the `OpenShift Console` switch to the Developer Perspective from the menu on the top left if it's not already selected:

image::s2i/dev-view.png[Developer Perspective]

[tabs, subs="attributes+,+macros"]
====

Use pre-built container::
+
--
We will create the model client using a pre-built container from a public container registry.

* From the `+Add` menu, click on the `Container images` tile:

image::s2i/from-registry.png[Add from registry, 800]

* In the ``Image name from external registry`` field, enter: `+quay.io/mmurakam/object-detection-rest:v0.1.0+`

[.lines_space]
[.console-input]
[source, text]
----
quay.io/mmurakam/object-detection-rest:v0.1.0
----

* You will see that OpenShift automatically validates the container image.

* Under Runtime icon, select `python`.
--

Build from source::
+
--
We will build the model client container on the cluster by referencing the git repository that contains the Dockerfile.

* From the `+Add` menu, click on the `Import from Git` tile:

image::s2i/from-git.png[Add from git, 800]


* In the ``Git Repo URL`` field, enter: `+https://github.com/mamurak/object-detection-rest.git+`

[.lines_space]
[.console-input]
[source,text]
----
https://github.com/mamurak/object-detection-rest.git
----

* You will see that OpenShift automatically recognized that our repo contains a Dockerfile, which can be used to build the container image.

image::s2i/docker.png[Builder Image, 800]
--
====

=== Customize the App Names

* Let's customize the name of our application.  Instead of `object-detection-rest-git-app` and `object-detection-rest-git`, let's change the *Application name* to `object-detection` and the *Name* to  `object-detection-rest`.  Later when we call our service internally from the same project, we'll use this name and the service port, `http://object-detection-rest:8080`.

[.lines_space]
[.console-input]
[source,text]
----
object-detection
----


[.lines_space]
[.console-input]
[source,text]
----
object-detection-rest
----

image::s2i/general.png[General Options, 700]

=== Route

This controls the external access to the REST API. Expand the *Show Advanced Routing options*

image::s2i/advanced-route-options.png[advanced route, 700]

Then make sure that the *Secure Route* option is unchecked, since we need the
object detection REST API to be available as an HTTP service.

image::s2i/unsecure-route.png[unsecure route, 700]


=== Resource Limits

.Troubleshooting Memory Issues
====
Instead of receiving Out of memory (OOMKill) on the OpenShift pod, the application will not start correctly.
If the memory limit is too low, *Gunicorn* will kill the worker process that is taking up too much memory as a runaway.
This will result in messages in the log that look like:
```
[2022-07-07 16:20:01 +0000] [1] [WARNING] Worker with pid 2135 was terminated due to signal 9
[2022-07-07 16:20:01 +0000] [2144] [INFO] Booting worker with pid: 2144
```
In these cases it's recommended to increase your pod deployment's memory limit.
====

=== Configure the Deployment Environment Variables

The application needs to know where to send its images for predictions. Earlier, we confirmed the inference endpoint of the model deployment.  We'll use that value.  This will tell our client service where to send the images by setting the variable `PREDICTION_URL` to the model's _Inference endpoint_.

* Open the *Deployment*  options to set an environment variable.

Name:
[.lines_space]
[.console-input]
[source,text]
----
PREDICTION_URL
----
Value:
[.lines_space]
[.console-input]
[source,text]
----
<Your model's inference endpoint you copied from your RHOAI project under Models and model servers>
----

image::s2i/env_prediction_url.png[Prediction URL for the inference endpoint]

//image::s2i/deployment-settings.png[]

=== Create

Everything is ready, so you can click on *Create*:

//image::s2i/create-button.png[alt text]

== Checking your Application 

* If you're building the application container from source, you will see that a build is going on from the build icon:

image::s2i/topology.png[alt text, 400]

* Click on the python logo in the center of the deployment to open a detail panel on the right. The automated
building process will take a few minutes. Some alerts (such as ImgPullBackOff) may appear while the first build
is still running, but that's OK. Then OpenShift will deploy the application (rollout).  When it's complete you can
see the build is complete and the pod is running.

image::s2i/topology-rest-complete.png[alt text, 700]

Once the build is complete xref:2-03-testing-deployment.adoc[head to the next section.]
