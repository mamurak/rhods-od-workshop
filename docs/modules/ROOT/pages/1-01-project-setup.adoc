= 1.1 Setting up your Data Science Project

include::_attributes.adoc[]

xref:index.adoc[Back to the introduction]

== The OpenShift Data Science Dashboard

NOTE: We're going to refer to _Red Hat OpenShift Data Science_ as _RHODS_ throughout this workshop.

You should be logged into *Red Hat OpenShift Data Science*, and be able to see the dashboard, that looks like this:

image::notebooks/rhods_dashboard.png[alt text]

*Red Hat OpenShift Data Science* brings you a variety of containerized on-demand environments, such as Jupyter Notebooks. Don't worry if you've never used notebooks before as this workshop will start with a small tutorial on what they are and how to use them.

* Now that you are logged into to *Red Hat OpenShift Data Science*, navigate to `Data Science Projects` in the left menu. Here you can see the list of existing projects that you have access to.

NOTE: Projects allow you and your team to organize and collaborate on resources within separated namespaces.

* There is one Data Science Project that has been prepared already for you, having the same name like your username. Enter the project by clicking on its name. You can now see its initial state. There are five types of Project resources, some of which have been pre-provisioned for your convenience and which we will review throughout the next steps:

** *Workbenches* are instances of your development and experimentation environment. They typically contain IDEs such as JupyterLab, RStudio and Visual Studio Code.

** A *Cluster storage* is a volume that persists the files and data you're working on within a workbench. A workbench has access to one or more cluster storage instances. One a volume named _development_ was already provisioned ahead of the workshop for your convenience.

** *Data connections* contain configuration parameters that are required to connect to a data source such as an S3 object bucket. Two data connections named _object-detection_ and _pipelines_ were already provisioned ahead of the workshop for your convenience.

** *Pipelines* contain the Data Science Pipelines that have been executed within the Project.

** *Models and model servers* allow you to quickly serve a trained model for real-time inference. You can have multiple model servers per Project, and one model server can host multiple models.

* Let's create a workbench. Click on _Create_ in the workbench section to proceed to the workbench configuration page. Choose the following settings:

** Choose an arbitrary *name* such as _object-detection-workbench_.

** Under *Notebook image* select `Object detection`.

** Under *Deployment size* select `Small`.

** Leave the *Environment variables* section empty. We will populate the workbench's environment variables through a data connection later.

** Under *Cluster storage* select `Use existing persistent storage` and select _development_, a volume that was provisioned ahead of the workshop for your convenience.

** Under *Data connections* check the box `Use a data connection`, select `Use existing data storage` and then choose _object-detection_, an S3 bucket that was provisioned ahead of the workshop for your convenience.

** Click *Create workbench* to initialize your workbench based on your selected configuration.

* You're redirected to the Project page where you can see the status of your new workbench. The first time it's initialized it will take about 1-2 minutes to start up. Subsequent starts will be faster.

To recap
* All files you create in your workbench will be persisted in the cluster storage volume.

* The data connection named _object-detection_ points to an S3 bucket where the data and models are stored, while the _pipelines_ data connection will hold metadate and logs from the Data Science Pipelines runs.

Before we continue, let's inspect the parameters of the _object-detection_ data connection that has been prepared for you.

* Under *Data connections*, find _object-detection_ and select `Edit data connection` under the *vertical ellipsis*, *â‹®*. In the configuration page, you can review the typical fields for an S3 connection. When done hit `Cancel` or simply close the window.

If you are curious about where the *Cluster storage* is managed, you can make use the `OpenShift Console`. To switch to it, click on the _add-ons_ icon (9 small squares) in the top right toolbar and select `OpenShift Console` to open it in a new tab.

* Ensure you're seeing the Administrator Perspective by selecting `Administrator` in the top field of the left menu. The cluster storage volume allocated to your Data Science Project can be found under `Storage` and the `PersistentVolumeClaims`. You will see that in addition to the _development_ volume, there are others which have been prepared and will be leverage in the next chapters of the workshop.

NOTE: OpenShift is the hybrid cloud Kubernetes-based engine that powers the RHODS platform and provides all the needed resources like storage, memory, CPUs and GPUs, as well as the DevSecOps and GitOps pipeline capabilities needed for Data Science Pipelines and MLOps flows. In the OpenShift Console you have two perspectives, `Administrator` and `Developer`, between which you can easily switch by selecting `Developer` or `Administrator` respectively in the top field of the left menu. Here you can inspect and edit all resources that you create throughout the workshop in the RHODS dashboard, as part of your Data Science Projects.

We'll use the OpenShift Console tab to deploy the object detection app later in this workshop. For now, keep the tab open for reference and return to the RHODS Dashboard tab.

NOTE: If you're at the OpenShift Web Console and need to navigate back to the OpenShift Data Science Dashboard, use the *Application Switcher* icon in the top right of the navigation bar.

image::notebooks/ocp-console-app-switcher.png[alt text, 400]

You're now all set. You got a bit familiar with the 'Red Hat OpenShift Data Science' dashboard as well as the `OpenShift Console`. You've also configured your first Data Science Project!

To start working with the object detection model, select the `Open` link next to your running workbench instance and
xref:1-02-jupyter-env.adoc[head to the next section.]
